---
category: [積體電路]
tags: [IoT, 數學]
title: 递归最小二乘算法
date: 2025-01-21 1:00:00
---

<style>
  table {
    width: 100%
    }
  td {
    vertical-align: center;
    text-align: center;
  }
  table.inputT{
    margin: 10px;
    width: auto;
    margin-left: auto;
    margin-right: auto;
    border: none;
  }
  input{
    text-align: center;
    padding: 0px 10px;
  }
  iframe{
    width: 100%;
    display: block;
    border-style:none;
  }
</style>

# 最小二乘算法

​二乘其实是指平方的意思，为什么用平方呢？因为平方可以消除误差正负方向上的差异，单纯的只比较长度。


![Alt X](../assets/img/math/lsqmethod.png)


如上图所示：蓝点是真实数据，黄点是每个真实数据的估计值，红线的长短即代表真实与估计距离，目标就是找到一条直线（模型）使得所有红线累和最短，推广到多维空间，就是找到一个超平面，而这个超平面是有数学公式解的！以下就是该公式一般化的推导过程。

基本方程为：

$$
y=mx
$$

调整后之最佳方式：

$$
\tilde{y} = m \tilde{x}
$$

误差方程为：

$$
\epsilon = y - \tilde{y} = y - m \tilde{x}
$$

$$
\epsilon(\tilde{x} |m, y)=(m \tilde{x} -y)^T(m \tilde{x} -y)
$$

其最优解为：

$$
\tilde{x} = (m^T m)^{-1}m^T y
$$

其中
 - $m$ 为样本输入矩阵。
 - $y$ 向量为求解函数 $f(x)$。
 -  $\tilde{y}$ 向量为调整后之最佳方式 y 值。
 - $\tilde{x}$ 向量为调整后之 x 平均值。

例子：

$$
y=mx+c
$$

$$
m=\begin{bmatrix} 1 \\ . \\ . \\ . \\ 1 \end{bmatrix}
$$

其最优解如上：

$$
\tilde{x} = (m^T m)^{-1}m^T y
$$

$$
(m^T m)^{-1} = \frac{1}{m}
$$

$$
\tilde{x} = \frac{\sum y}{m} = A
$$

# 递归最小二乘算法

标量测量 x 和 y 以及未知参数 m 和 c 为 

$$ y=mx $$ 

上文公式（8）可得，

$$
\tilde{x} = A = (m_n^T m_n)^{-1}m_n^T y_n，R_n = m^T_n m_n $$

$$z_n = m^Ty = \sum _{i=1}^{n} x_i y_i 
$$

$$
A = (R_n)^{-1} z_n
$$

当在 $ n+1 $ 时如下：

$$ A_{n+1} =  (R_{n+1})^{-1} z_{n+1} = (m_{n+1}^T m_{n+1})^{-1}m_{n+1}^T y_{n+1} $$

备注：很多时候都会讲到最小二乘，它又叫做最小平方法，即求得其的最小平方和，比如通过最小二乘，得到模型的残差 **bias** 最小, 即最小化目标值与模型预估值的距离的平方和）


在现实中，新的数据往往会比旧的数据更有意义，所以需要引入一个遗忘因子 $\lambda \in [0,1]$（一般在 **0.98** 到 **1** 之间选择），来评估数据对当前模型的影响，使得越往后数据影响越大（这个实际是加权最小二乘的内容）。


## 递归最小二乘算法公式


$$
y= mx+c \\
A_{n+1}=A_n+K_n(y_n-mA)
$$


$$
R_{n+1} = \lambda R_n + x_{n+1} \cdot (x_{n+1})^{-1}
$$


在以上表达式中，比较麻烦的是求 $(R_{n+1})^{-1} $，如果得到它，那么就能成功求得需要计算的线性模型。

$$
K_n = \frac{P_n\,\phi_{n+1}}{1 + \phi_{n+1}^\top\,P_n\,\phi_{n+1}}
$$

$$
P_{n+1} = P_n - K_n\\phi_{n+1}^\top\,P_n
$$

$$
\hat{\theta}_{n+1} =
$$

$$
\hat{\theta}_{n} + K_n (z_{n+1} - \phi_{n+1}^\top\,\hat{\theta}_n)
$$


在讨论矢量之前需要先理解以人上公式。

假设想找到 n 个数字的平均值。称之为 $A_n$

$$
A_n = \frac{x_1+x_2+\cdots x_n}{n}
$$

现在想象一下已经计算出 $A_n$，并且现在收到一个新数据。n+1 个数字的平均值是多少？

$$
A_{n+1} = \frac{x_1+x_2+\cdots x_n+x_{n+1}}{n+1}
$$

关键在于不必从计算 $A_{n+1}$。也可以将上述等式重写为:

$$
\sum_{i=1}^{n+1} x_i=x_1+x_2+\cdots x_n+x_{n+1} \\
= \left(x_1+x_2+\cdots x_n\right)+x_{n+1}=n\times A_n+x_{n+1}
$$

重新排列并简化后得到

$$
A_{n+1}= A_n + \frac{1}{n+1} \left(A_{n+1}-A_n\right)
$$

这是递归定义。它显示了如何使用每个新数据值更新平均值。

$$
A_{n+1}  = A_{n} + K \left(A_n - x_{n+1}\right)
$$

上面的等式有两个重要部分。
 - $K$ 称为增益。
 - $ A_n - x_{n+1} $ 称为创新

是期望的值和实际值之间的差值。注意：$K$ 取决于已经处理的样本数量。

现在对于递归线性方程（将写为 $ y=mx+c $），具有相同的结构：

$$
\begin{bmatrix}m_{n+1} \\ c_{n+1} \end{bmatrix}=\begin{bmatrix}m_n \\ c_n \end{bmatrix} +
\begin{bmatrix} K_{11} & K_{12} \\ K_{21} & K_{22}\end{bmatrix}\left(y_{n+1} - (m_n x_{n+1} + c_n)\right)
$$

基本上是用已计的数 $ m_n，c_n $ 求出新的 $ m_{n+1}，c_{n+1} $。只需要输入 $ y_{n+1} $ 即测量值及 $ x_{n+1} $ 即可。


$K$ 的公式使用矩阵求逆引理，该引理给出了 $K$ 的递归公式。实际计算非常繁琐。给你概念是与任何算法一样，实际细节都是代数。以下是过程：
 - 写出 $n$ 个数据点的公式和 $n+1$ 个数据点的公式。
 - 在 $n+1$ 个数据点的公式中，用 $n$ 个数据点的公式替换所有涉及前 $n$ 个数据点的表达式
 - 重新排列并简化
 - 最终会得到形式为表达式，其中 $v$ 是一个向量如下。 


$$
H^{-1}-(H+v v^T)^{-1}
$$
 - 利用矩阵求逆引理得到公式如下，事实上，写出 $H^{−1}$ 的递归关系。

$$
H^{-1}-(H+v v^T)^{-1}=\frac{H^{-1}vv^TH^{-1}}{(1+v^T H^{-1} v)}
$$

矩阵增益 $K$ 可以用 $H$ 的形式写出。

递归最小二乘算法是最佳线性无偏估计的一个特例，又是卡尔曼滤波器的一个特例。在更广泛的背景下提出了递归公式。